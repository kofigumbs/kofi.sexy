---
title: 'Typebeat, checkpoint 0.1.0'
categories: ['blog']

---

I occasionally [make music](/music/tide).
And like many a programmer-turned-music-maker, my musical process often devolves into writing code to optimize various parts of my workflow.
Typebeat is my latest contribution to that tradition.
It's a virtual [groovebox](https://en.wikipedia.org/wiki/Groovebox) that's entirely keyboard-operated:

<div style="aspect-ratio:1/1;">
  <iframe
    src="https://www.youtube-nocookie.com/embed/RT0qUB4gbas"
    title="A screen recording demonstrating how I create music with Typebeat. The app is laid out like a QWERTY keyboard with labels for each key. When I press a key on my keyboard (shown in a video overlay) the corresponding key in the app is illuminated and its effect is triggered."
    style="width:100%;height:100%;"
    frameborder="0"
    allowfullscreen
  ></iframe>
</div>

You can play with a _pre-alpha_ demo at [typebeat.kofi.sexy](https://typebeat.kofi.sexy).
I think it would be fun to release a more substantial version someday, but that version would look quite a bit different than it does now.
So I figured it would be worth taking a breather and reflecting on how far the project has come.

Typebeat's UI is inspired by two computing environments that make me feel productive and creative.
The first is [live-coding](https://livecode.nyc), the performance art of writing code to generate media.
I love watching live-coding shows because the performers typically insist on showing their work.
The code is as much a part of the performance as the resulting visuals and audio.
Live-coders use their input device as their instrument by creating programs that the computer then interprets.
I think of Typebeat as starting with that idea, keyboard as instrument, and making the analogy more direct.
Press `N` to play a drum kick.
Press `K` to play a clap.
Type `N`-`Y`-`K`-`Y` to play a "boots and cats" house beat.

Of course this sort of direct mapping doesn't scale: there are more musical commands than there are keys on a keyboard.
So Typebeat also takes inspiration from [vi](https://en.wikipedia.org/wiki/Vi) and incorporates a modal interface.
In Audition Mode, Typebeat works as I mentioned above, with each key triggering a different sound.
But in Note Mode, Typebeat transposes the active sound, letting you access notes in a 2-octave range.
Send Mode lets you route your sound to different effect buses, and then Return Mode lets you configure what each of those effect buses actually do.
Like my experience with vi, Typebeat is incredibly fast to navigate once learned.

I'm calling this post a checkpoint and not a release because I think from here, I'll adjust my heading a bit.
Focusing on the keyboard input device was a helpful design constraint, but I think I insisted on it too heavily.
Sometimes, particularly when I _don't_ know exactly what I want to hear, playing with knobs and sliders helps me to discover new ideas.
Nudging on-screen values with a keyboard does not produce that same effect for me.
Similarly, some ideas are just difficult to fit onto a screen if I insist that everything must be viewed as if it were literally a keyboard.
I'm looking forward to experimenting more with analog input devices and figuring out how to merge those with my typist-oriented workflow.
